# wildberries scraper

Сканер для сбора сведений о товарах с www.wildberries.ru и www.ozon.ru


## Установка

1. Должен быть установлен docker
2. Загрузить репозиторий в папку wondersell_scraper
3. Отредактировать файл wondersell_scraper/proxy_list.csv , добавив туда несколько прокси серверов. Например:
```
protocol;ip;port;login;passoword
socks;83.217.11.126;2324;sdfgdsfg;asdfhjkasl
socks;185.58.204.145;4555;sdfert;dfghwr
socks;185.211.246.79;4664;sdfgs;andfgdhsrew
```

4. (new!) Установка:

```sh
$ cd wondersell_scraper
$ docker pull selenoid/vnc_chrome:81.0
$ docker-compose build
```
5. Запуск:
```sh
$ docker-compose up
```
после удачного запуска можно подключиться в контейнеру
для этого в новом терминале!:
```sh
$ docker exec -it wondersellscraper_scrab_1 bash
```
должно появиться приглашение на подобии
```sh
root@f96349b4e0d9:/data#
```
## Работа
(все команды выполняются в контейнере)
1. Для начала нужно проверить прокси сервера
```sh
# scrapy crawl proxy_test
```
! при первом запуске после создания контейнера могут появиться ошибки:
(Error occurred during fetching http://useragentstring.com/pages/useragentstring.php?name=Chrome
...
socket.timeout: timed out)
но при этом все должно работать штатно и при повторе такого быть не должно.
в результат увидим количество рабочих прокси
(по умолчанию программа выбирает только протокол socks!)
Более подробно о параметрах сканеров - в разделе "Параметры"

```sh
....
[2020-04-03 10:37:33,218][INFO] count proxy : 3
```
если count proxy > 0 , можно работать дальше

2. Сканирование каталога
```sh
# scrapy crawl catalog
```
после завершения сканер печатает статистику. например:
```sh
stat:
{
    "downloader/request_bytes": 1012,
    "downloader/request_count": 3,
    "downloader/request_method_count/GET": 3,
    "downloader/response_bytes": 5282,
    "downloader/response_count": 3,
    "downloader/response_status_count/200": 3,
    "elapsed_time_seconds": 6.515392,
    "finish_reason": "finished",
    "finish_time": "2020-04-03T10:37:33.216861",
    "log_count/DEBUG": 1,
    "log_count/INFO": 15,
    "memusage/max": 69128192,
    "memusage/startup": 69128192,
    "response_received_count": 3,
    "scheduler/dequeued": 3,
    "scheduler/dequeued/memory": 3,
    "scheduler/enqueued": 3,
    "scheduler/enqueued/memory": 3,
    "start_time": "2020-04-03T10:37:26.701469"
}
```
3. Теперь можно сканировать товары

```sh
# scrapy crawl wb -a catalog_ids=endpoints -a limit=1
```
эта команда возьмет одну категорию и соберет все товары из нее.
Прервать процесс сканирования: Cntl+c (Внимание! после одного нажатия процесс еще будет продолжаться некоторое время. Нужно дождаться отчета!)

4. Сбор цен и остатков
```sh
# scrapy crawl wb_cost -a item_ids=all
```
этот сканер тоже можно остановит Cntl+c

5. Доступ к базе.

По умолчанию подключена база mysql
в локальном терминале введите
```sh
$ mysql -h 127.0.0.1 -P 3307 -u root -p
```
пароль: root
используйте стандартные команды sql, например:
```sh
sqlite> use wildsearch
sqlite> SELECT id, url FROM catalog limit 10;
```

## Параметры
### Сканеры
все сканеры основаны на фреймворке Scrapy. Большинство параметров можно почитать в официальной документации. Здесь приводятся только специфические для проекта.

#### # scrapy crawl catalog
сканирует и собирает url-ы всех категорий
- запускается без специальных параметров
- остановка на паузу не возможна

особенности работы:
В начале работы сканер устанавливает всем записям recheck_found = False,
собирает все каталоги, потом присваивает upper_id.

#### # scrapy crawl wb
сканирует и собирает общую информацию о товарах
- без параметров не сканирует
- поддерживаются комбинации параметров
- *-a item_id* (=[1,2,3],[4-10]|all)
сканирует по товарам, которые уже есть в базе, принимает id товара через запятую и диапазоном или *all* например:
```sh
# scrapy crawl wb -а item_id=1,2,3,4,25-50,100-500
# scrapy crawl wb -а item_id=all
```
- *-a item_art* (=[1,2,3])
по аналогии с item_id, принимает артикул товара
- *-a item_cat_id* (=[1,2,3],[4-10])
по аналогии с item_id, принимает id категории товара
- *-a cat_id* (=[1,2,3],[4-10]|endpoints)
сканирует категории, которые уже есть в базе, находит в них новые товары. вместо *all* рекомендую использовать *endpoints*
- *-a limit* (=1)
используется для тестирования

- *-a overwrite* (=true|false) по умолчанию false
перезаписывает name, specification, img_urls и seller

- поддерживает установку на паузу

особенности работы:
параметр endpoints выбирает только те категории, которые не являются ни кому родителями. Все элементы, у которых нет upper_id, а таких довольно много, в эту выборку не попадают.

#### # scrapy crawl wb_cost
- без параметров не сканирует
- поддерживаются комбинации параметров
- поддерживает установку на паузу
принимает параметры item_id, item_art и item_cat_id по аналогии с *scrapy crawl wb*

особенности работы:
При работе сканера можно часто увидеть ошибку- не найдено такое-то поле. Это нормально.

#### # scrapy crawl proxy_test
Загружает прокси сервера, тестирует их и ставит в очередь "ротатора". Если прокси не отвечает, то автоматически удаляется из очереди.
- можно запускать без параметров
- *-a source* (= csv|scylla|none) по умолчанию csv
указывает источник загрузки списка прокси серверов в БД.
csv- файл, путь к которому указан в PROXY_LIST_CSV в settings.py
scylla- сервис для автоматического поиска прокси серверов (подробности ниже)
none- не загружает новые адреса
 - *-a mode* (=reload|reset|current) по умолчанию reload
режим обновления очереди адресов (из БД в очередь).
reload- обновляет очередь новыми адресами.
reset- удаляет очередь и загружает заново.
current- оставляет очередь без изменений
 - *-a protocol* (=https|socks|http|all) по умолчанию socks
 - *-a group* (=<название>) по умолчанию None
protocol и group являются фильтрами, которые применяет загрузчик очереди.
group также работает в паре с параметром csv - если указано имя группы, то севера загруженные из файла попадают в БД с этим именем, а затем отбираются для очереди.

Все умолчания можно переопределить в файле настроек.

особенности работы:
Прокси ротатор использует Redis для хранения списка серверов. При каждом запросе сканера ротатор выдает очередной адрес (и так по кругу). Если адрес не отвечает, ротатор удаляет его из очереди, таким образом список может опустеть. Если адреса работают нормально, этот список остается в памяти до тех пор пока его не удалят командой reset (см. параметр mode).
Основная база прокси серверов хранится в БД в таблице proxy.

###  (new!) Сканеры для ozon.ru
#### # scrapy crawl oz_catalog
работает так же как *scrapy crawl catalog*

#### # scrapy crawl oz
работете также как *scrapy crawl wb*

#### # scrapy crawl oz_qty
собирает информацию об остатках
параметры также как в *scrapy crawl wb_cost*
 - дополнительные параметры (готовятся к публикации)


### Использование паузы
сканеры *wd*, *wd_cost* и *oz*  поддерживают установку на паузу.
если планируется прерывание скрипта с возможностью продолжения нужно добавить параметр
- *-s JOBDIR (=<path>)*
<path> - путь к каталогу с архивом экземпляра сканера
например:
```sh
# scrapy crawl wb -a catalog_ids=endpoints -s JOBDIR=crawls/wb_1
```
эта команда создаст папку архива в корне директории.
если остановить такой сканер, то можно потом продолжить введя команду с тем же архивом.
Внимание! Остановка сканера (с помощь Cntl+c) не завершат процесс сразу. Сканер еще некоторое время работает сохраняя результаты. Если нажать Cntl+c повторно, это прервет работу сканера и повредит архив, что сделает невозможным продолжить с того же места.

### Настройки
Путь к файлу настроек scrapy: ./wildsearch_crawler/settings.py

#### Подключение к MySql
В сборке база данных работает как отдельный контейнер.
Для внешнего подключения к базе  проброшен порт 3307 (127.0.0.1:3307), логин и пароль root,
Если нужно использовать другие базы - измените параметры WS_DB_ENGINE (для wildsearch) и OZ_DB_ENGINE (для ozon) в settings.py.
#### Настройки задержки и скорости сканирования
в settings.py по умолчанию значения:
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 0
AUTOTHROTTLE_MAX_DELAY = 300
AUTOTHROTTLE_TARGET_CONCURRENCY = 2.0
AUTOTHROTTLE_DEBUG = True
документация здесь:
https://docs.scrapy.org/en/latest/topics/autothrottle.html


#### (new!) Сервис Selenoid-ui
Подключен для сканера oz_qty
Доступен по адресу http://127.0.0.1:8080
(Инструкции готовятся к публикации)


#### Сервис Scylla
Автоматический поиск публичных прокси серверов.
По умолчанию сервис отключен. Чтобы включить, нужно раскомментировать соответствующий раздел раздел в файле docker-compose.yml и перезапустить контейнеры.
документация здесь:
https://github.com/imWildCat/scylla


# Что нового

### v-0.1.0
 - Добавлен сканер для ozon.ru

### v-0.0.1
 - Решена проблема с конфликтом redis и scrapyd при старте системы
 - Добавлены новые параметры для запуска сканеров
 - Изменена структура БД
